---
title: "Quantium Virtual Intership Virtual Strategy and Analytics - Task 1"
author: Anas Marwan
date: 21/1/2023
output:
    pdf_document:
        df_print: default
        highlight: tango
        keep_tex: yes
        latex_engine: pdflatex
    header-includes:
        \usepackage{fvextra}
        \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
---

# Solution Task 1

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(linewidth=80)
```


#### Load required libraries

```{r 0 Load Libraries, results = 'hide'}
library(data.table)
library(ggplot2)
library(ggmosaic)
library(readr)
library(readxl)
```

```{r}
customerData <- read.csv('QVI_purchase_behaviour.csv')
transactionData <- read_excel('QVI_transaction_data.xlsx')
```

## Explatory Data Analysis

The first step of analysis is to understand the data. Let's take a look at each of the datasets provided.

### Examining transaction data

```{r}
head(transactionData)
```

```{r}
str(transactionData)
```

It occurs that the date was in integer format. To clean this, we need to convert this to the date format. We first note that the date format begin on 30 Dec 1899 and it's definitely safe to assume our dataset starts after that date.

```{r Convert DATE to date format}
transactionData$DATE <- as.Date(transactionData$DATE, origin = "1899-12-30")
head(transactionData)
```

Now, the date is in the appropriate format and more readable than the previous one. Next, we want to be sure that we are looking at only potato chips. Let's check the summary of PROD_NAME.

```{r}
#### Examine the class and mode of PROD_NAME
summary_prodname <- summary(transactionData$PROD_NAME) 
summary_prodname
#### Examine the different products of chips
unique_prodname <- unique(transactionData$PROD_NAME)
unique_prodname
length(unique_prodname)
```

There were exactly 114 different products in our dataset. We need to verify that they were all chips, not others.

```{r Further examine PROD_NAME}
#### Examine the words in PROD_NAME to see if there are any incorrect entries 
#### such as products that are not chips 
productWords <- data.table(unlist(strsplit(transactionData$PROD_NAME, " "))) 
setnames(productWords, 'words')
productWords
```

We are only interested in knowing chips name, so the numbers and special characters are not necessary for this examination. Let's remove them.

```{r, message=FALSE}
#### replacing digits
productWords1 <- chartr("0123456789", "&&&&&&&&&&", productWords)
```

```{r, message=FALSE}
#### removing non-alphanumeric
productWords2 <- gsub('[^[:alnum:] ]','',productWords1)
```

```{r Putting the cleaned strings into data.table}
productname <- data.table(unlist(strsplit(productWords2, " ")))
productname
```

Now, we identify the most frequent word occurs in PROD_NAME

```{r Converting data.table to dataframe}
productname <- as.data.frame(productname)
productname
```

```{r Counting the frequency of the words and sort them from high to low frequency}
library(dplyr)
productname %>%
  filter(V1 != "") %>%
  group_by(V1) %>%
  count(V1) %>%
  arrange(desc(n))
```

As we can see above, the word "g" happens to be the most appeared, however this character was part of the weight of the product and was not remove during the deletion of digits. Thus, we may ignore "g". This makes "Chips" as the most frequent words in PROD_NAME column. Going through the first two pages, one will eventually found that there is "Salsa" product. This is not part of our use later, so we shall remove it.

```{r Remove salsa product}
#### Remove salsa products
transactionData <- as.data.table(transactionData)
transactionData[, SALSA := grepl("salsa", tolower(PROD_NAME))] 
transactionData <- transactionData[SALSA == FALSE, ][, SALSA := NULL]
```

So now we have cleaned the date format and removed non-chips products, we proceed to check the numerical values to seek for missing values or outliers.

```{r Summary of numerical data}
sum(is.na(transactionData))
summary(transactionData)
```

Fortunately, there is no missing values in our dataset. However, judging from the interquartile range and min-max values of PROD_QTY and TOT_SALES, we might have a potential outlier. Let's investigate when the purchased quantity equals 200.

```{r Checking potential outliers}
transactionData[PROD_QTY > 10,]
```

We found that 2 transactions where the quantity equals 200 by the same customer (LYTL_CARD_NBR). Let's check if this customer had made any other transactions for chips.

```{r Checking a customer activities}
transactionData[LYLTY_CARD_NBR == "226000",]
```

There were no other purchases made by the customer, so this customer is not an ordinary customer. It can be that the purchases made were for commercial. Thus, we will remove the Loyalty card number form our analysis.

```{r Filter out loyalty card number}
#### we can either filter by LYLTY_CARD_NBR!='226000' or PROD_QTY<200, both will do the work.
transactionData <- transactionData[LYLTY_CARD_NBR != '226000',]
```

Let's take a glimpse of our current transactionData

```{r Re-examine transactionData}
str(transactionData)
summary(transactionData)
head(transactionData)
```

That looks much better. Now, let's look at the number of transaction lines over time to see if there are any obvious data issues such as missing data.

```{r Transactions by date}
#### Count transactions by date
transactionbyDate <- transactionData %>%
  group_by(DATE) %>%
  count(DATE) 
```

There are only 364 rows which means 1 date is missing. Let's try to find the missing date. To do this, we create a sequence of dates of 365 days from 01-07-2018 to 30-06-2019. Then, we merge (left-join) the sequence with transactionbyDate to check the date with missing values

```{r, include=FALSE}
library(data.table)
library(table.express)
```

```{r Finding missing date}
#### Creating the sequence of date
daySeq <- seq.Date(as.Date("2018-07-01"), as.Date("2019-06-30"), by="days")
daySeq <- as.data.table(daySeq)
setnames(daySeq, "date")
#### left-joining daySeq and transactionbyDate
transactionbyDate <- as.data.table(transactionbyDate)
day_n <- daySeq %>% left_join(transactionbyDate, date = DATE)
day_n[which(is.na(day_n$n)),]
```

Aha! The missing date happens to be the Christmas day, which all stores were close on that day. Let's see the plot of the purchases in December.

```{r Plotting purchases over time in December barplot}
#### Setting plot themes to format graphs 
theme_set(theme_bw()) 
theme_update(plot.title = element_text(hjust = 0.5)) 
#### Plot transactions over time 
ggplot(day_n, aes(x = DATE, y = n)) + geom_line() + labs(x = "Day", y = "Number of transactions", title = "Transactions over time") + scale_x_date(breaks = "1 month") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5))

#### Plot transactions in December
day_n_Dec <- day_n[month(DATE) == 12,]
ggplot(data=day_n_Dec, aes(x=DATE, y=n)) +  labs(x = "Day", y = "Number of transactions", title = "Transactions over time") +
  geom_bar(stat="identity", fill="steelblue")+
  theme_minimal()
```

Now, we are happy that there were no outliers in our dataset, we can move on to creating other features such as brand of chips or pack size form PROD_NAME. We will start with pack size.

```{r Create pack size}
#### Pack size 
#### We can work this out by taking the digits that are in PROD_NAME 
transactionData[, PACK_SIZE := parse_number(PROD_NAME)]
summary(transactionData)
```
We have the maximum pack size is 380g and the minimum pack size is 70g - looks fine!

Let's plot a histogram showing pack number of transactions over pack size.

```{r Histogram N over pack size}
#### create a histogram using ggplot
ggplot(transactionData, aes(x=PACK_SIZE)) + geom_histogram(bins=30,color="black", fill="blue")
```

The pack size counts looks alright.

Now, we move on to the second new feature, which is the brand name. To do that, we extract the brand name from the column PROD_NAME

```{r Create brand name}
#### Install and import stringr
library(stringr)
#### defining BRAND feature for transactionData
transactionData[, BRAND := word(PROD_NAME, 1)]
```

Let's take a look at the values of our new column BRAND

```{r Checking BRAND column}
unique(transactionData$BRAND)
```
It looks like some of the values are shortforms of another. We need to standardize this.

```{r Cleaning BRAND column}
transactionData[BRAND == "Red", BRAND := "RRD"]
transactionData[BRAND == "Dorito", BRAND := "Doritos"]
transactionData[BRAND == "Natural", BRAND := "NCC"]
transactionData[BRAND == "Snbts", BRAND := "Sunbites"]
transactionData[BRAND == "Smith", BRAND := "Smiths"]
transactionData[BRAND == "Infzns", BRAND := "Infuzions"]
transactionData[BRAND == "Grain", BRAND := "GrainWaves"]
transactionData[BRAND == "GrnWves", BRAND := "GrainWaves"]
transactionData[BRAND == "French", BRAND := "FrenchFries"]
```

```{r Rechecking BRAND column}
unique(transactionData$BRAND)
```

It looks like we have 21 different brand of chips. Let's plot a barplot of transactions over brand

```{r Barplot transactions over brand}
ggplot(transactionData, aes(x=BRAND)) + geom_bar(color="black", fill="blue")  + theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
```


Let's examine our transactionData one more time.

```{r examination on transactionData}
str(transactionData)
head(transactionData)
summary(transactionData)
```

Everything seems fine. Now, we move on to the next dataset, customerData.

## Examining customerData

Let's examine the customerData.

```{r Viewing customerData}
str(customerData)
summary(customerData)
head(customerData)
```
We have 3 features in customerData, namely LYLTY_CARD_NBR, LIFESTAGE, and PREMIUM_CUSTOMER. Let's look at the values they take in. 

```{r Checking features in customerData}
#### Checking missing values
sum(is.na(customerData))
#### checking the length of unique customers, distinct values of premium and lifestage.
length(unique(customerData$LYLTY_CARD_NBR))
unique(customerData$LIFESTAGE)
unique(customerData$PREMIUM_CUSTOMER)
```
To summarize what we just did, we've checked that there is no missing value, the LYLTY_CARD_NBR column has unique values, 7 different values for LIFESTAGE, and 3 different type of PREMIUM_CUSTOMER. The values look fine, and we don't need to do much cleaning. Now, let's merge customerData to transactionData.

```{r Merging customerData to transactionData}
#### Merge transaction data to customer data 
data <- merge(transactionData, customerData, all.x = TRUE)
str(data)
head(data)
```
As the number of rows in `data` is the same as that of `transactionData`, we can be sure that no duplicates were created. This is because we created `data` by setting `all.x = TRUE` (in other words, a left join) which means take all the rows in `transactionData` and find rows with matching values in shared columns and then joining the details in these rows to the `x` or the first mentioned table.

Let's also check if some customers were not matched on by checking for nulls.

```{r Check for missing customer details}
#### Examine the rows with no LIFESTAGE data from customerData dataset.
which(is.na(data$LIFESTAGE))
```
Great, there is no null values! So all our customers in the transaction data has been accounted for in the customer dataset.

Let's save the "data" dataset in a csv file. this will be good for Task 2.

```{r Code to save dataset as a csv, eval=FALSE} 
# fwrite(data, paste0('E:/data-portfolio/quantium-data-analytics-VI/',"QVI_data.csv")) 
```

Data Exploration is now complete!

## Data Analysis on customer section

Now that the data is ready for analysis, we can define some metrics of interest to the client:

- Who spends the most on chips (total sales), describing customers by lifestage and how premium their general purchasing behaviour is
- How many customers are in each segment
- How many chips are bought per customer by segment- What's the average chip price by customer segment 

We could also ask our data team for more information. Examples are:
- The customer's total spend over the period and total spend for each transaction to understand what proportion of their grocery spend is on chips
- Proportion of customers in each customer segment overall to compare against the mix of customers who purchase chips 

Let's start with calculating total sales by LIFESTAGE and PREMIUM_CUSTOMER and plotting the split by these segments to describe which customer segment contribute most to chip sales.

```{r  fig.width = 10, fig.align = "center"}
#### Total sales by LIFESTAGE and PREMIUM_CUSTOMER 
sales_by_lifestage <- data %>%
  group_by(LIFESTAGE) %>%
  summarise(sum(TOT_SALES))

sales_by_premium <- data %>%
  group_by(PREMIUM_CUSTOMER) %>%
  summarise(sum(TOT_SALES))

#### Plot them
ggplot(sales_by_lifestage, aes(x=LIFESTAGE, y=V1)) + geom_col(width=0.5, color="black", fill="blue") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5)) + labs(x = "Lifestage", y = "Number of transactions", title = "Transactions by Lifestage")
ggplot(sales_by_premium, aes(x=PREMIUM_CUSTOMER, y=V1)) + geom_col(width=0.3, color="black", fill="blue")  + theme(axis.text.x = element_text(angle = 90, vjust = 0.5)) + labs(x = "Premium Class", y = "Number of transactions", title = "Transactions by Premium")
```

There are more Mainstream - young singles/couples and Mainstream - retirees who buy chips. This contributes to there being more sales to these customer segments but this is not a major driver for the Budget - Older families segment. 

Higher sales may also be driven by more units of chips being bought per customer. Let's have a look at this next.

```{r fig.width = 10, fig.align = "center"} 
#### Average number of units per customer by LIFESTAGE and PREMIUM_CUSTOMER 

######## Create a table of customer with total quantity purchased
total_qty <- data %>%
  group_by(LYLTY_CARD_NBR) %>%
  summarise(TOT_PROD_QTY=sum(PROD_QTY))

total_qty <- merge(total_qty, customerData, all.x=TRUE)

######## Summarise by LIFESTAGE and PREMIUM_CUSTOMER

tot_qty_lifestyle <- total_qty %>% group_by(LIFESTAGE) %>% summarise(AVG_PROD_QTY=mean(TOT_PROD_QTY))
tot_qty_premium <- total_qty %>% group_by(PREMIUM_CUSTOMER) %>% summarise(AVG_PROD_QTY=mean(TOT_PROD_QTY))

######## Plot them

ggplot(tot_qty_lifestyle, aes(x=LIFESTAGE, y=AVG_PROD_QTY)) + geom_col(width=0.5, color="black", fill="blue") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5)) + labs(x = "Lifestage", y = "Average Quantity Purchase", title = "Average Quantity Purchase by Lifestage")
ggplot(tot_qty_premium, aes(x=PREMIUM_CUSTOMER, y=AVG_PROD_QTY)) + geom_col(width=0.3, color="black", fill="blue")  + theme(axis.text.x = element_text(angle = 90, vjust = 0.5)) + labs(x = "Premium Class", y = "Average Quantity Purchase", title = "Average Quantity Purchase by Premium")
```
Older families and young families in general buy more chips per customer 

Let's also investigate the average price per unit chips bought for each customer segment as this is also a driver of total sales.

```{r fig.width = 10, fig.align = "center"} 
#### Average price per unit by LIFESTAGE and PREMIUM_CUSTOMER 

# Over to you! Calculate and plot the average price per unit sold (average sale price) by those two customer dimensions. 
# Create table of customer and sales per unit
# First we summarise TOTAL_SALES per customer and then combine with total_qty table.
sales_per_customer <- data %>% group_by(LYLTY_CARD_NBR) %>% summarise(CUSTOMER_SALES = sum(TOT_SALES))
sales_per_customer <- merge(total_qty, sales_per_customer, all.x = TRUE)

# add a column that represent price per unit paid bu each customer
sales_per_unit <- sales_per_customer %>% mutate(SPU=CUSTOMER_SALES/TOT_PROD_QTY)

# summarise average price per unit by lifestage and premium categories.
spu <- sales_per_unit %>% group_by(LIFESTAGE, PREMIUM_CUSTOMER) %>% summarise(AVG_SPU=mean(SPU))
######## Plot them

ggplot(spu, aes(fill=PREMIUM_CUSTOMER, y=AVG_SPU, x=LIFESTAGE))  + 
    geom_bar(position="dodge", stat="identity") + theme(axis.text.x = element_text(angle = 60, vjust = 0.5)) + labs(x = "Lifestage", y = "Average Price Per Unit", title = "Average Purchase Per Unit by Lifestage and Premium")
```
Mainstream midage and young singles and couples are more willing to pay more per packet of chips compared to their budget and premium counterparts. This may be due to premium shoppers being more likely to buy healthy snacks and when they buy chips, this is mainly for entertainment purposes rather than their own consumption. This is also supported by there being fewer premium midage and young singles and couples buying chips compared to their mainstream counterparts. 

As the difference in average price per unit isn't large, we can check if this difference is statistically different.

```{r} 
#### Perform an independent t-test between mainstream vs premium and budget midage and 
#### young singles and couples 

mainstream <- (spu %>% filter(PREMIUM_CUSTOMER=="Mainstream" & (LIFESTAGE=="YOUNG SINGLES/COUPLES"|LIFESTAGE=="MIDAGE SINGLES/COUPLES")))$AVG_SPU
budget <- (spu %>% filter(PREMIUM_CUSTOMER =="Budget" & (LIFESTAGE=="YOUNG SINGLES/COUPLES"|LIFESTAGE=="MIDAGE SINGLES/COUPLES")))$AVG_SPU
premium <- (spu %>% filter(PREMIUM_CUSTOMER =="Premium" & (LIFESTAGE=="YOUNG SINGLES/COUPLES"|LIFESTAGE=="MIDAGE SINGLES/COUPLES")))$AVG_SPU

t.test(mainstream, budget, var.equal=TRUE)
t.test(mainstream, premium, var.equal=TRUE)
```
The t-test results in a p-value of 0.04258 (0.01667), i.e. the unit price for mainstream, young and mid-age singles and couples ARE significantly higher than that of budget (premium), young and midage singles and couples.

## Deep dive into specific customer segments for insights 

We have found quite a few interesting insights that we can dive deeper into. We might want to target customer segments that contribute the most to sales to retain them or further increase sales. Let's look at Mainstream - young singles/couples. For instance, let's find out if they tend to buy a particular brand of chips.

```{r fig.align = "center", warning=FALSE} 
#### Deep dive into Mainstream, young singles/couples 
#### Mainstreams

# filter data into Mainstreams only
mainstreamData <- data %>% filter(PREMIUM_CUSTOMER == "Mainstream")

# frequency of brands purchased by mainstreams
brands_mainstream <- mainstreamData %>% count(BRAND, sort=TRUE)

# plot 
ggplot(brands_mainstream, aes(x=BRAND, y=n)) + geom_col(width = 0.5, color="black", fill="blue") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5))


#### Young singles/couples

# filter data into Young singles/couples only
youngData <- data %>% filter(LIFESTAGE == "YOUNG SINGLES/COUPLES")

# frequency of brands purchased by mainstreams
brands_young <- youngData %>% count(BRAND, sort=TRUE)

# plot 
ggplot(brands_young, aes(x=BRAND, y=n)) + geom_col(width = 0.5, color="black", fill="blue") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
```
We can see that among the customers in the two groups, Kettle leading the charts followed by Smiths, Doritos and Pringles. 

Let's also find out if our target segment tends to buy larger packs of chips.

```{r fig.align = "center", warning=FALSE} 
#### Deep dive into Mainstream, young singles/couples 

#### Mainstreams

# change datatype of PACK_SIZE to numeric
mainstreamData$PACK_SIZE <- as.character(mainstreamData$PACK_SIZE)

# frequency of packsize purchased by mainstreams
size_mainstream <- mainstreamData %>% count(PACK_SIZE, sort=TRUE)

# plot 
ggplot(size_mainstream, aes(x=PACK_SIZE, y=n)) + geom_col(width = 0.5, color="black", fill="blue") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5))


#### Young singles/couples

# change datatype of PACK_SIZE to numeric
youngData$PACK_SIZE <- as.character(youngData$PACK_SIZE)

# frequency of brands purchased by mainstreams
size_young <- youngData %>% count(PACK_SIZE, sort=TRUE)

# plot 
ggplot(size_young, aes(x=PACK_SIZE, y=n)) + geom_col(width = 0.5, color="black", fill="blue") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
```
As the plot suggests, the most popular pack size among our targets is 175g, followed by 150g, 134g, and 110g. However, we might want to understand how strongly related pack size and brands are, since pack size corresponds to some products. Not all products available in different pack size. See the following tables.

```{r}
data %>% filter(PACK_SIZE==175) %>% count(BRAND, sort = TRUE)
data %>% filter(BRAND=="Kettle") %>% count(PACK_SIZE, sort = TRUE)
```

Motivated by this, we might want to consider brand+pack size as a category.

```{r, fig.align='center', warning=FALSE}
mainstream_BS <- mainstreamData %>% mutate(BRAND_SIZE=paste(BRAND, PACK_SIZE)) %>% count(BRAND_SIZE, sort = TRUE)
ggplot(mainstream_BS, aes(x=BRAND_SIZE, y=n)) + geom_col(width = 0.5, color="black", fill="blue") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5))

young_BS <- youngData %>% mutate(BRAND_SIZE=paste(BRAND, PACK_SIZE)) %>% count(BRAND_SIZE, sort = TRUE)
ggplot(young_BS, aes(x=BRAND_SIZE, y=n)) + geom_col(width = 0.5, color="black", fill="blue") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
```
This is interesting. It turns out that Pringles 134 is the best sale in the target groups. Which product(s) Pringles 134 corresponds to?

```{r}
unique((mainstreamData %>% filter(BRAND == "Pringles" & PACK_SIZE == "134"))$PROD_NAME)
unique((youngData %>% filter(BRAND == "Pringles" & PACK_SIZE == "134"))$PROD_NAME)
```
This must be a popular series of Pringles chips. Let's find out which flavour is the most popular.

```{r}
MS_pringles <- mainstreamData %>% filter(BRAND == "Pringles" & PACK_SIZE == "134") %>% count(PROD_NAME)
ggplot(MS_pringles, aes(x=PROD_NAME, y=n)) + geom_col(width = 0.5, color="black", fill="blue") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
young_pringles <- youngData %>% filter(BRAND == "Pringles" & PACK_SIZE == "134") %>% count(PROD_NAME)
ggplot(young_pringles, aes(x=PROD_NAME, y=n)) + geom_col(width = 0.5, color="black", fill="blue") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
```
It seems that all the flavors are almost equally popular. Let's end our analysis here.





