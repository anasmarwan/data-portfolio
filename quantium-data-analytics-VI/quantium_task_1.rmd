---
title: "Quantium Virtual Intership Virtual Strategy and Analytics - Task 1"
author: Anas Marwan
date: 21/1/2023
mainfont: Roboto
monofont:Consolas
output:
    pdf_document:
        df_print: default
        highlight: tango
        keep_tex: yes
        latex_engine: xelatex
    header-includes:
        \usepackage{fvextra}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
---

# Solution Task 1

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(linewidth=80)
```

```{r knitr line wrap setup, include=FALSE}
hook_output = knit_hooks$get("output")
knit_hooks$set(output = function(x, options)
{
  if (!is.null(n <- options$linewidth))
  {
    x = knitr:::split_lines(x)
    if (any(nchar(x) > n))
      x =  strwrap(x, width = n)
    x = paste(x, collapse = "\n")
  }
  hooks_output(x, options)
})
```

#### Load required libraries

```{r 0 Load Libraries, results = 'hide'}
library(data.table)
library(ggplot2)
library(ggmosaic)
library(readr)
install.packages('readxl')
library(readxl)
```

```{r}
customerData <- read.csv('QVI_purchase_behaviour.csv')
transactionData <- read_excel('QVI_transaction_data.xlsx')
```

## Explatory Data Analysis

The first step of analysis is to understand the data. Let's take a look at each of the datasets provided.

### Examining transaction data

```{r Examining transaction data}
#### observing the first few rows of the dataset
head(transactionData)
```

```{r}
str(transactionData)
```

It occurs that the date was in integer format. To clean this, we need to convert this to the date format. We first note that the date format begin on 30 Dec 1899 and it's definitely safe to assume our dataset starts after that date.

```{r Convert DATE to date format}
transactionData$DATE <- as.Date(transactionData$DATE, origin = "1899-12-30")
head(transactionData)
```

Now, the date is in the appropriate format and more readable than the previous one. Next, we want to be sure that we are looking at only potato chips. Let's check the summary of PROD_NAME.

```{r}
#### Examine the class and mode of PROD_NAME
summary_prodname <- summary(transactionData$PROD_NAME) 
summary_prodname
#### Examine the different products of chips
unique_prodname <- unique(transactionData$PROD_NAME)
unique_prodname
length(unique_prodname)
```

There were exactly 114 different products in our dataset. We need to verify that they were all chips, not others.

```{r Further examine PROD_NAME}
#### Examine the words in PROD_NAME to see if there are any incorrect entries 
#### such as products that are not chips 
productWords <- data.table(unlist(strsplit(transactionData$PROD_NAME, " "))) 
setnames(productWords, 'words')
productWords
```

We are only interested in knowing chips name, so the numbers and special characters are not necessary for this examination. Let's remove them.

```{r}
#### replacing digits
productWords1 <- chartr("0123456789", "&&&&&&&&&&", productWords)
productWords1
```

```{r}
#### removing non-alphanumeric
productWords2 <- gsub('[^[:alnum:] ]','',productWords1)
productWords2
```

```{r Putting the cleaned strings into data.table}
productname <- data.table(unlist(strsplit(productWords2, " ")))
productname
```

Now, we identify the most frequent word occurs in PROD_NAME

```{r Converting data.table to dataframe}
productname <- as.data.frame(productname)
productname
```

```{r Counting the frequency of the words and sort them from high to low frequency}
library(dplyr)
productname %>%
  filter(V1 != "") %>%
  group_by(V1) %>%
  count(V1) %>%
  arrange(desc(n))
```

As we can see above, the word "g" happens to be the most appeared, however this character was part of the weight of the product and was not remove during the deletion of digits. Thus, we may ignore "g". This makes "Chips" as the most frequent words in PROD_NAME column. Going through the first two pages, one will eventually found that there is "Salsa" product. This is not part of our use later, so we shall remove it.

```{r Remove salsa product}
#### Remove salsa products
transactionData <- as.data.table(transactionData)
transactionData[, SALSA := grepl("salsa", tolower(PROD_NAME))] 
transactionData <- transactionData[SALSA == FALSE, ][, SALSA := NULL]
```

So now we have cleaned the date format and removed non-chips products, we proceed to check the numerical values to seek for missing values or outliers.

```{r Summary of numerical data}
sum(is.na(transactionData))
summary(transactionData)
```

Fortunately, there is no missing values in our dataset. However, judging from the interquartile range and min-max values of PROD_QTY and TOT_SALES, we might have a potential outlier. Let's investigate when the purchased quantity equals 200.

```{r Checking potential outliers}
transactionData[PROD_QTY > 10,]
```

We found that 2 transactions where the quantity equals 200 by the same customer (LYTL_CARD_NBR). Let's check if this customer had made any other transactions for chips.

```{r Checking a customer activities}
transactionData[LYLTY_CARD_NBR == "226000",]
```

There were no other purchases made by the customer, so this customer is not an ordinary customer. It can be that the purchases made were for commercial. Thus, we will remove the Loyalty card number form our analysis.

```{r Filter out loyalty card number}
#### we can either filter by LYLTY_CARD_NBR!='226000' or PROD_QTY<200, both will do the work.
transactionData <- transactionData[LYLTY_CARD_NBR != '226000',]
```

Let's take a glimpse of our current transactionData

```{r Re-examine transactionData}
str(transactionData)
summary(transactionData)
head(transactionData)
```

That looks much better. Now, let's look at the number of transaction lines over time to see if there are any obvious data issues such as missing data.

```{r Transactions by date}
#### Count transactions by date
transactionbyDate <- transactionData %>%
  group_by(DATE) %>%
  count(DATE) 
```

There are only 364 rows which means 1 date is missing. Let's try to find the missing date. To do this, we create a sequence of dates of 365 days from 01-07-2018 to 30-06-2019. Then, we merge (left-join) the sequence with transactionbyDate to check the date with missing values

```{r}
library(data.table)
install.packages("table.express")
library(table.express)
```

```{r Finding missing date}
#### Creating the sequence of date
daySeq <- seq.Date(as.Date("2018-07-01"), as.Date("2019-06-30"), by="days")
daySeq <- as.data.table(daySeq)
setnames(daySeq, "date")
#### left-joining daySeq and transactionbyDate
transactionbyDate <- as.data.table(transactionbyDate)
day_n <- daySeq %>% left_join(transactionbyDate, date = DATE)
day_n[which(is.na(day_n$n)),]
```

Aha! The missing date happens to be the Christmas day, which all stores were close on that day. Let's see the plot of the purchases in December.

```{r Plotting purchases over time in December barplot}
#### Setting plot themes to format graphs 
theme_set(theme_bw()) 
theme_update(plot.title = element_text(hjust = 0.5)) 
#### Plot transactions over time 
ggplot(day_n, aes(x = DATE, y = n)) + geom_line() + labs(x = "Day", y = "Number of transactions", title = "Transactions over time") + scale_x_date(breaks = "1 month") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5))

#### Plot transactions in December
day_n_Dec <- day_n[month(DATE) == 12,]
ggplot(data=day_n_Dec, aes(x=DATE, y=n)) +  labs(x = "Day", y = "Number of transactions", title = "Transactions over time") +
  geom_bar(stat="identity", fill="steelblue")+
  theme_minimal()
```

Now, we are happy that there were no outliers in our dataset, we can move on to creating other features such as brand of chips or pack size form PROD_NAME. We will start with pack size.

```{r Create pack size}
#### Pack size 
#### We can work this out by taking the digits that are in PROD_NAME 
transactionData[, PACK_SIZE := parse_number(PROD_NAME)]
summary(transactionData)
```
We have the maximum pack size is 380g and the minimum pack size is 70g - looks fine!

Let's plot a histogram showing pack number of transactions over pack size.

```{r Histogram N over pack size}
#### create a histogram using ggplot
ggplot(transactionData, aes(x=PACK_SIZE)) + geom_histogram(bins=30,color="black", fill="blue")
```

The pack size counts looks alright.

Now, we move on to the second new feature, which is the brand name. To do that, we extract the brand name from the column PROD_NAME

```{r Create brand name}
#### Install and import stringr
install.packages("stringr")
library(stringr)
#### defining BRAND feature for transactionData
transactionData[, BRAND := word(PROD_NAME, 1)]
```

Let's take a look at the values of our new column BRAND

```{r Checking BRAND column}
unique(transactionData$BRAND)
```
It looks like some of the values are shortforms of another. We need to standardize this.

```{r Cleaning BRAND column}
transactionData[BRAND == "Red", BRAND := "RRD"]
transactionData[BRAND == "Dorito", BRAND := "Doritos"]
transactionData[BRAND == "Natural", BRAND := "NCC"]
transactionData[BRAND == "Snbts", BRAND := "Sunbites"]
transactionData[BRAND == "Smith", BRAND := "Smiths"]
transactionData[BRAND == "Infzns", BRAND := "Infuzions"]
transactionData[BRAND == "Grain", BRAND := "GrainWaves"]
transactionData[BRAND == "GrnWves", BRAND := "GrainWaves"]
transactionData[BRAND == "French", BRAND := "FrenchFries"]
```

```{r Rechecking BRAND column}
unique(transactionData$BRAND)
```

It looks like we have 21 different brand of chips. Let's plot a barplot of transactions over brand

```{r Barplot transactions over brand}
ggplot(transactionData, aes(x=BRAND)) + geom_bar(color="black", fill="blue")  + theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
```


Let's examine our transactionData one more time.

```{r examination on transactionData}
str(transactionData)
head(transactionData)
summary(transactionData)
```

Everything seems fine. Now, we move on to the next dataset, customerData.

## Examining customerData

Let's overview the customerData.

```{r Viewing customerData}
str(customerData)
summary(customerData)
head(customerData)
```
We have 3 features in customerData, namely LYLTY_CARD_NBR, LIFESTAGE, and PREMIUM_CUSTOMER. Let's look at the values they take in. 

```{r Checking features in customerData}
#### Checking missing values
sum(is.na(customerData))
#### checking the length of unique customers, distinct values of premium and lifestage.
length(unique(customerData$LYLTY_CARD_NBR))
unique(customerData$LIFESTAGE)
unique(customerData$PREMIUM_CUSTOMER)
```
To summarize what we just did, we've checked that there is no missing value, the LYLTY_CARD_NBR column has unique values, 7 different values for LIFESTAGE, and 3 different type of PREMIUM_CUSTOMER. The values look fine, and we don't need to do much cleaning. Now, let's merge customerData to transactionData.

```{r Merging customerData to transactionData}
#### Merge transaction data to customer data 
data <- merge(transactionData, customerData, all.x = TRUE)
str(data)
head(data)
```
As the number of rows in `data` is the same as that of `transactionData`, we can be sure that no duplicates were created. This is because we created `data` by setting `all.x = TRUE` (in other words, a left join) which means take all the rows in `transactionData` and find rows with matching values in shared columns and then joining the details in these rows to the `x` or the first mentioned table.

Let's also check if some customers were not matched on by checking for nulls.

```{r Check for missing customer details}
#### Examine the rows with no LIFESTAGE data from customerData dataset.
which(is.na(data$LIFESTAGE))
```
Great, there is no null values! So all our customers in the transaction data has been accounted for in the customer dataset.

Let's save the "data" dataset in a csv file. this will be good for Task 2.

```{r Code to save dataset as a csv, eval=FALSE} 
# fwrite(data, paste0('E:/data-portfolio/quantium-data-analytics-VI/',"QVI_data.csv")) 
```

Data Exploration is now complete!

## Data Analysis on customer section

Now that the data is ready for analysis, we can define some metrics of interest to the client:

- Who spends the most on chips (total sales), describing customers by lifestage and how premium their general purchasing behaviour is
- How many customers are in each segment
- How many chips are bought per customer by segment- What's the average chip price by customer segment 

We could also ask our data team for more information. Examples are:
- The customer's total spend over the period and total spend for each transaction to understand what proportion of their grocery spend is on chips
- Proportion of customers in each customer segment overall to compare against the mix of customers who purchase chips 

Let's start with calculating total sales by LIFESTAGE and PREMIUM_CUSTOMER and plotting the split by these segments to describe which customer segment contribute most to chip sales.

```{r  fig.width = 10, fig.align = "center"}
#### Total sales by LIFESTAGE and PREMIUM_CUSTOMER 
sales_by_lifestage <- data %>%
  group_by(LIFESTAGE) %>%
  summarise(sum(TOT_SALES))

sales_by_premium <- data %>%
  group_by(PREMIUM_CUSTOMER) %>%
  summarise(sum(TOT_SALES))

#### Plot them
ggplot(sales_by_lifestage, aes(x=LIFESTAGE, y=V1)) + geom_col(bins= 20, color="black", fill="blue") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
ggplot(sales_by_premium, aes(x=PREMIUM_CUSTOMER, y=V1)) + geom_col(bins= 20, color="black", fill="blue")  + theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
```




